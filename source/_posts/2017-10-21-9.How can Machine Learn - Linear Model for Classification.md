---
layout: post
title: 9.How can Machine Learn? - Linear Model for Classification
categories: [ReadNote]
tags: ReadNote-Machine-Learning-Foundation
---


# How can Machine Learn? - Linear Model for Classification
----------------------------------
<!-- TOC depthFrom:1 depthTo:6 withLinks:1 updateOnSave:1 orderedList:0 -->

- [How can Machine Learn? - Linear Model for Classification](#how-can-machine-learn-linear-model-for-classification)
	- [1. Linear Models for Binary Classification](#1-linear-models-for-binary-classification)
		- [1) Analyzing of Three Linear Models](#1-analyzing-of-three-linear-models)
		- [2) Error Functions Comparison](#2-error-functions-comparison)
	- [2. Multiclass via Logistic Regression - OVA Algorithm](#2-multiclass-via-logistic-regression-ova-algorithm)
	- [3. Multiclass via Binary Classification - OVO Algorithm](#3-multiclass-via-binary-classification-ovo-algorithm)
	- [Summary](#summary)
	- [Reference](#reference)

<!-- /TOC -->

<br><br>
----------------------------------
>主要讨论 Linear Classification, Linear Reegression, Logistic Regression 在分类问题上的优劣对比，并拓展到多元分类

## 1. Linear Models for Binary Classification

### 1) Analyzing of Three Linear Models
我们目前学习了Classification, Linear Reegression, Logistic Regression， 这三个模型有很多类似的地方。
总结如图一，这里引入了s(score)作为得分，$s=w^Tx$

![Linear Models Revisit](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/0e01f9a531b829ec48a4f3e9970fc737f62f9b56/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-1%20Linear%20Models.png)
<center> 图一 Linear Models Revisit <sup>[1]</sup></center>
<br>

从图中可以看出，Linear Classification的求解是NP-hard问题，其他两个方法都容易求解。并且可以看到他们的Hypothesis 都与 s 有关。所以可以利用这两种模型的算法近似求得二分类问题的最优权值w




### 2) Error Functions Comparison

1.首先对比3种方法的误差方程，如图二所示

![Error Functions Revisit](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/f0bb56f85acd2e6c8500017ff01411c065136237/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-2%20Error%20Function%20Revisit.png)
<center> 图二 Error Functions Revisit <sup>[1]</sup></center>
<br>



2.根据图二的错误函数得到图三的关系图。

![Visualizing Error Functions](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/89a29e7078c423404f64048c7ebbef7c33c6f534/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-3%20Visualizing%20Error%20Functions.png)
<center> 图三 Visualizing Error Functions <sup>[1]</sup></center>
<br>

从图三，我们可以得到以下几个结论：
1. 对于 $err_{0/1}$， 在 $ys > 0$ 和 $ys < 0$ 的值相差很大
2. $err_{CE}$ 随着 ys的增大而减小，而且无线接近于0，在 $ys > 0$ 的情况下，逐渐毕竟于 $err_{0/1}$
3. $err_{SQR}$ 是一个凹函数，在 $ys = 1$ 的情况下，与 $err_{0/1}$ 相等
4. $err_{0/1}$  始终小于 $err_{SQR}$， 绝大部分情况下小于 $err_{CE}$
5. 虽然$err_{0/1}$  始终小于 $err_{SQR}$， 但是两者的差距较大
6. 虽然$err_{0/1}$  不是全部情况下小于 $err_{CE}$，但是稍微做调整，可以实现 $err_{0/1}$  始终小于 $err_{CE}$ ，并且这两者的曲线接近，所以我们下面将想办法采用这种方式。

err为了让 $err_{0/1}$  也始终小于 $err_{CE}$ ，我们稍微做一些调整, 得到scaled CE $err_{SCE}$，如图四所示，这里只是给  $err_{CE}$ 的表达式把对数从 $log_e$ 换成 $log_2$。即从公式（1）换成公式（2）

$$
err_{CE} = err(w,x,y) = err_{CE}(s,y) = ln(1+exp(-yw^Tx)) = ln(1+exp(-ys))
\tag{1}
$$

$$
err_{SCE} = \frac{1}{ln2} err_{CE} = log_2(1+exp(-ys))
\tag{2}
$$

![Visualizing Error Functions with Scaled CE](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/0277ba5bccfbf510f13974f587508dbebd21bea0/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-4%20Visualizing%20Error%20Functions-scaled%20ce.png)
<center> 图四 Visualizing Error Functions with Scaled CE <sup>[1]</sup></center>
<br>

从图四中，我们可以发现，$err_{0/1}$  始终小于 $err_{SQR}$ 和 $err_{CE}$，如公式（3）所示，那么在数据量足够大的情况下，$E_{out}$ 也有类似的情况，如公式（4）所示。再接着，由于之前VC Bound的结论，我们可以得到公式（5）， 所以接下来我们就可以Logistic Regrssion的方法去解决Linear Classification的问题了

$$
E_{in}^{0/1}(w) \leq E_{in}^{SCE}(w) = \frac{1}{ln2} E_{in}^{CE}(w)
\tag{3}
$$
$$
E_{out}^{0/1}(w) \leq E_{out}^{SCE}(w) = \frac{1}{ln2} E_{out}^{CE}(w)
\tag{4}
$$
$$
E_{in}^{0/1}(w) \leq E_{in}^{0/1}(w) + \Omega ^{0/1}  \leq E_{out}^{SCE}(w)  + \Omega ^{0/1} = \frac{1}{ln2} E_{out}^{CE}(w)  + \Omega ^{0/1}
\tag{5}
$$

算法流程一般是在输出空间{-1, +1} 的情况下，通过线性回归和logistic回归相对应的求解方法求出最优的权值 $W_{REG}$；
将求得的代入公式sign，得到最优假设函数。




<br><br>
----------------------------------
## 2. Multiclass via Logistic Regression - OVA Algorithm

实际生活中，也有很多的场合需要对多种情况进行分类：比如区分病人患了哪种类型的疾病；区分不同种类的蔬菜等。

求解多类别问题可以采用二元分类的思想，将多类问题分解多多个二元分类问题，然后再分别求权值，最终分到最高权值的类别去。

如图五的多类别问题，我们可以分解成图六的多个二元分类问题去求解，最终中间的公共区域，我们通过公式（6）求得最大的概率，哪一类的概率最大，就把这个点归于哪一类。
>要注意使用软分类（因为直接分类的话中间的公共区域无法区分）

$$
\DeclareMathOperator*{\argmax}{argmax}
\begin{equation}
g(x) = argmax_{k ∈ y} (\theta w^T_{[k]} x)
\end{equation}
\tag{6}
$$

![Multiclass Classification](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/bcdd07e44255ec22600bb992b5928f177522630e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-5%20Multiclass%20Classification.png)
<center> 图五 Multiclass Classification <sup>[2]</sup></center>
<br>

![Multiclass Classification Soft Classifiers](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/3d035b4130569658df8474312c2d45e632fdfc79/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-6%20Multiclass%20Classification%20Soft%20Classifiers.png)
<center> 图六 Multiclass Classification Soft Classifiers <sup>[2]</sup></center>
<br>

这种算法称作一对多（One-Versus-All），简称为OVA，表示一个类别对其他所有类别。

算法的流程如图七所示

![OVA Decomposition](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/378a48ebf7a50d47599da2296c24c90e68ebd10c/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-7%20OVA%20Decomposition.png)
<center> 图七 OVA Decomposition <sup>[2]</sup></center>
<br>

该算法的优点是简单有效，易于类似于logistic函数的二元分类问题扩展成多类别分类；缺点是当类别特别多时，产生了不平衡的现象（如类别特别多，则+1的数据量就很少，大部分都是-1，数据量严重不平衡）。



<br><br>
----------------------------------
## 3. Multiclass via Binary Classification - OVO Algorithm
上一节的最后提到OVA的方式在类别非常多的情况下，出现了训练数据严重失衡的现象，于是有另外一种方法 一对一(One-Versus-One)算法，简称OVO。同样对图五的问题做多元分类，但是这次我们一次单独考虑2种类型的点，直到两两都进行过分类（分类次数就是组合数 $C$， 例子中就是 $C_4^2 = 6$）。如图八所示。

![OVO](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/15e07a2ab76b1bdb9f632949833e71c801f1371e/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-8%20OVO.png)
<center> 图八 OVO <sup>[3]</sup></center>
<br>


算法的流程如图九所示

![OVO Decomposition](https://raw.githubusercontent.com/JasonDean-1/MarkdownPhoto/0bd54f49a461dcdf1bf0b3f50c11ebdb6401472a/MachineLearning/Machine%20Learning%20Foundation%20--%20Hsuan-Tien%20Lin%20in%20NTU/chapter9-9%20OVO%20Decomposition.png)
<center> 图九 OVO <sup>[3]</sup></center>
<br>

其优点是简单有效，在做两两对比时，每次使用的不是全部训练数据，而是仅属于当前两类的训练数据，能将所有类似于二元分类的算法扩展成多元分类问题； 缺点是对比次数是 O(N^2) 即：$C_k^2$。 也就是说要分的类别越多，就需要花费更多的存储空间和运算时间。

## Summary
1. 首先我们对比了目前学到的Linear Models，由于Linear Classification的NP Hard求解问题，我们分析，最终发现可以使用Logistic Regression 的方法进行求解
2. 接着我们讨论了Multiclass Classification的 OVA 算法，但是该算法存在问题：类型很多的时候，会出现数据不平衡的问题
3. 最后在OVA的基础上，我们继续讨论了OVO算法，该算法克服了OVA数据不平衡的问题。但是该算法的缺点是存储空间和运算时间都比较大


<br><br>
----------------------------------

## Reference
[1] 机器学习基石(台湾大学-林轩田)\11\11 - 1 - Linear Models for Binary Classification (21-35)

[2] 机器学习基石(台湾大学-林轩田)\11\11 - 3 - Multiclass via Logistic Regression (14-18)

[3] 机器学习基石(台湾大学-林轩田)\11\11 - 4 - Multiclass via Binary Classification (11-35)
